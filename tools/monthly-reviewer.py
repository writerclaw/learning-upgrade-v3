#!/usr/bin/env python3
"""
æ¯æœˆå¤ç›˜åˆ†æå™¨ v3.0
åŠŸèƒ½ï¼š
  1. åŠ è½½ä¸Šæœˆå…¨éƒ¨å‘¨æŠ¥
  2. æœˆåº¦è¶‹åŠ¿åˆ†æ
  3. çŸ¥è¯†å›¾è°± & æˆé•¿è·¯å¾„
  4. ä¸‹æœˆå­¦ä¹ è§„åˆ’å»ºè®®
  5. Notion æœˆæŠ¥é¡µé¢
  6. Telegram æ¨é€
"""

import json
import os
import ssl
import sys
import urllib.request
from datetime import datetime, timedelta
from pathlib import Path
import calendar

# === è·¯å¾„é…ç½® ===
WORKSPACE_DIR = Path("/home/writer/.openclaw/workspace")
LOGS_DIR = WORKSPACE_DIR / "logs"
SKILL_DIR = WORKSPACE_DIR / "skills" / "learning-upgrade"
TRACKER_DIR = SKILL_DIR / "tracker"
OUTPUT_DIR = LOGS_DIR / "monthly-review"

# === API é…ç½® ===
ARK_API_KEY = os.environ.get('ARK_API_KEY', '')
ARK_BASE_URL = "https://ark.cn-beijing.volces.com/api/v3"
MATON_API_KEY = os.environ.get('MATON_API_KEY', '')
MATON_BASE_URL = "https://gateway.maton.ai/v1"

# Notion æ ¹é¡µé¢ ID
LEARNING_DIARY_ROOT_ID = "1a09bfd6-0b4f-80d7-ab33-ca2e38e0d9f0"


def load_env():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_file = Path("/home/writer/.openclaw/.env")
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    if line.startswith('export '):
                        line = line[7:]
                    key, _, value = line.partition('=')
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    os.environ[key] = value

    global ARK_API_KEY, MATON_API_KEY
    ARK_API_KEY = os.environ.get('ARK_API_KEY', ARK_API_KEY)
    MATON_API_KEY = os.environ.get('MATON_API_KEY', MATON_API_KEY)


def get_last_month_info():
    """è·å–ä¸Šæœˆä¿¡æ¯"""
    today = datetime.now()
    # ä¸Šæœˆçš„ç¬¬ä¸€å¤©
    first_of_this_month = today.replace(day=1)
    last_day_of_prev = first_of_this_month - timedelta(days=1)
    first_of_prev = last_day_of_prev.replace(day=1)

    year = first_of_prev.year
    month = first_of_prev.month
    total_days = calendar.monthrange(year, month)[1]

    return {
        "year": year,
        "month": month,
        "year_month": f"{year}-{month:02d}",
        "year_month_cn": f"{year} å¹´ {month:02d} æœˆ",
        "first_day": first_of_prev,
        "last_day": last_day_of_prev,
        "total_days": total_days
    }


def get_weeks_in_month(year, month):
    """è·å–æŸæœˆåŒ…å«çš„ ISO å‘¨åˆ—è¡¨"""
    total_days = calendar.monthrange(year, month)[1]
    weeks = set()
    for day in range(1, total_days + 1):
        d = datetime(year, month, day)
        iso_week = d.isocalendar()[1]
        weeks.add(f"{year}-W{iso_week:02d}")
    return sorted(weeks)


def load_weekly_reports(year, month):
    """åŠ è½½æŸæœˆçš„æ‰€æœ‰å‘¨æŠ¥"""
    weeks = get_weeks_in_month(year, month)
    weekly_dir = LOGS_DIR / "weekly-review"
    reports = []

    for week_id in weeks:
        md_file = weekly_dir / f"{week_id}.md"
        json_file = weekly_dir / f"{week_id}.json"

        weekly = {"week_id": week_id, "content": None, "analysis": None}

        if md_file.exists():
            with open(md_file, 'r', encoding='utf-8') as f:
                weekly["content"] = f.read()

        if json_file.exists():
            with open(json_file, 'r', encoding='utf-8') as f:
                try:
                    weekly["analysis"] = json.load(f)
                except json.JSONDecodeError:
                    pass

        if weekly["content"] or weekly["analysis"]:
            reports.append(weekly)

    return reports


def load_daily_stats(year, month):
    """ç»Ÿè®¡æŸæœˆçš„æ¯æ—¥å­¦ä¹ æƒ…å†µ"""
    total_days = calendar.monthrange(year, month)[1]
    learning_days = 0
    max_streak = 0
    current_streak = 0

    for day in range(1, total_days + 1):
        date_stamp = f"{year}{month:02d}{day:02d}"
        has_report = False

        for subdir in ["github-monitor", "community-scraper", "tech-analyzer"]:
            log_dir = LOGS_DIR / subdir
            # æ£€æŸ¥å„ç§å¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
            for pattern in [f"{subdir}-{date_stamp}.md", f"tech-analysis-{date_stamp}.md"]:
                if (log_dir / pattern).exists():
                    has_report = True
                    break
            if has_report:
                break

        if has_report:
            learning_days += 1
            current_streak += 1
            max_streak = max(max_streak, current_streak)
        else:
            current_streak = 0

    return {
        "total_days": total_days,
        "learning_days": learning_days,
        "max_streak": max_streak,
        "rate": round(learning_days / total_days, 2)
    }


def load_monthly_action_items(year_month):
    """åŠ è½½æŸæœˆçš„è¡ŒåŠ¨é¡¹"""
    action_file = TRACKER_DIR / "action-items.json"
    if not action_file.exists():
        return {"items": [], "total": 0, "done": 0, "completion_rate": 0}

    with open(action_file, 'r', encoding='utf-8') as f:
        data = json.load(f)

    items = [
        i for i in data.get("items", [])
        if i.get("source_date", "").startswith(year_month)
    ]

    done = sum(1 for i in items if i["status"] == "done")
    dropped = sum(1 for i in items if i["status"] == "dropped")
    total_active = len(items) - dropped

    return {
        "items": items,
        "total": len(items),
        "done": done,
        "dropped": dropped,
        "pending": sum(1 for i in items if i["status"] in ("pending", "in_progress")),
        "completion_rate": round(done / max(total_active, 1), 2)
    }


def llm_monthly_analysis(weekly_reports, daily_stats, action_items, month_info):
    """è°ƒç”¨ LLM è¿›è¡Œæœˆåº¦ç»¼åˆåˆ†æ"""

    # æ±‡æ€»å‘¨æŠ¥å†…å®¹
    weekly_summaries = ""
    for wr in weekly_reports:
        if wr["content"]:
            weekly_summaries += f"\n--- {wr['week_id']} ---\n{wr['content'][:3000]}\n"

    prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±æŠ€æœ¯æˆé•¿é¡¾é—®ã€‚è¯·åŸºäºä»¥ä¸‹ä¸€ä¸ªæœˆçš„å­¦ä¹ æ•°æ®è¿›è¡Œå…¨é¢å¤ç›˜åˆ†æã€‚

## æœˆä»½: {month_info['year_month_cn']}

## å­¦ä¹ æŠ•å…¥ç»Ÿè®¡
- æ€»å¤©æ•°: {daily_stats['total_days']}
- å­¦ä¹ å¤©æ•°: {daily_stats['learning_days']}
- å­¦ä¹ ç‡: {daily_stats['rate'] * 100:.0f}%
- æœ€ä½³è¿ç»­å­¦ä¹ : {daily_stats['max_streak']} å¤©

## è¡ŒåŠ¨é¡¹ç»Ÿè®¡
- æ€»è®¡: {action_items['total']} é¡¹
- å·²å®Œæˆ: {action_items['done']} é¡¹
- å®Œæˆç‡: {action_items['completion_rate'] * 100:.0f}%

## å‘¨æŠ¥æ±‡æ€» ({len(weekly_reports)} å‘¨)
{weekly_summaries[:10000]}

## è¯·è¾“å‡ºä»¥ä¸‹åˆ†æ (JSON æ ¼å¼):

```json
{{
  "tech_evolution": [
    {{
      "week": "W01",
      "focus": "è¯¥å‘¨é‡ç‚¹å…³æ³¨çš„æŠ€æœ¯æ–¹å‘",
      "key_learning": "å…³é”®æ”¶è·"
    }}
  ],
  "source_quality": [
    {{
      "source": "ä¿¡æ¯æºåç§°",
      "value_count": æœ‰ä»·å€¼å†…å®¹æ•°é‡,
      "high_value_rate": 0.0åˆ°1.0,
      "rating": "1-5æ˜Ÿè¯„çº§",
      "suggestion": "æ”¹è¿›å»ºè®®"
    }}
  ],
  "knowledge_coverage": {{
    "covered_areas": ["å·²è¦†ç›–æŠ€æœ¯é¢†åŸŸ"],
    "deep_areas": ["æ·±åº¦å­¦ä¹ çš„é¢†åŸŸ"],
    "blind_spots": ["åº”è¯¥å…³æ³¨ä½†æœªå…³æ³¨çš„é¢†åŸŸ"],
    "depth_vs_breadth": "ä¸“ç²¾/å‡è¡¡/æ³›å­¦ çš„è¯„ä¼°"
  }},
  "growth_assessment": {{
    "overall_score": 0åˆ°100,
    "strengths": ["æœ¬æœˆåšå¾—å¥½çš„æ–¹é¢"],
    "weaknesses": ["éœ€è¦æ”¹è¿›çš„æ–¹é¢"],
    "growth_curve": "ä¸Šå‡/æŒå¹³/ä¸‹é™"
  }},
  "next_month_plan": {{
    "focus_directions": [
      {{
        "direction": "é‡ç‚¹æ–¹å‘",
        "reason": "ä¸ºä»€ä¹ˆæ¨è",
        "resources": ["æ¨èèµ„æº"]
      }}
    ],
    "monthly_challenge": {{
      "title": "æœˆåº¦æŒ‘æˆ˜ç›®æ ‡",
      "description": "å…·ä½“æè¿°",
      "success_criteria": "æˆåŠŸæ ‡å‡†"
    }},
    "avoid_pitfalls": ["éœ€è¦é¿å…çš„é—®é¢˜"]
  }}
}}
```
"""

    url = f"{ARK_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {ARK_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "glm-4.7",
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä½æŠ€æœ¯æˆé•¿å¯¼å¸ˆï¼Œæ“…é•¿ä»å¤§é‡å­¦ä¹ æ•°æ®ä¸­æç‚¼æˆé•¿æ´å¯Ÿå’Œå‘å±•å»ºè®®ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 5000
    }

    ctx = ssl.create_default_context()
    try:
        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )
        response = urllib.request.urlopen(req, context=ctx, timeout=300)
        result = json.load(response)
        content = result['choices'][0]['message']['content']

        import re
        json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
        if json_match:
            return json.loads(json_match.group(1))
        else:
            return json.loads(content)
    except Exception as e:
        print(f"âŒ LLM åˆ†æå¤±è´¥: {e}")
        return None


def generate_monthly_report(month_info, daily_stats, weekly_reports, action_items, llm_analysis):
    """ç”Ÿæˆæœˆåº¦å¤ç›˜ Markdown æŠ¥å‘Š"""

    md = []
    md.append(f"# ğŸ“ˆ {month_info['year_month_cn']} â€” æœˆåº¦å¤ç›˜")
    md.append("")
    md.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    md.append("")

    # æœˆåº¦ç»Ÿè®¡
    md.append("## ğŸ“Š æœˆåº¦ç»Ÿè®¡")
    md.append("")
    md.append(f"- æ€»å­¦ä¹ å¤©æ•°: **{daily_stats['learning_days']}/{daily_stats['total_days']}** ({daily_stats['rate'] * 100:.0f}%)")
    md.append(f"- å‘¨æŠ¥è¦†ç›–: **{len(weekly_reports)} å‘¨**")
    md.append(f"- è¡ŒåŠ¨é¡¹å®Œæˆç‡: **{action_items['completion_rate'] * 100:.0f}%** ({action_items['done']}/{action_items['total']})")
    md.append(f"- æœ€ä½³è¿ç»­å­¦ä¹ : **{daily_stats['max_streak']} å¤©**")
    md.append("")

    if llm_analysis:
        # æŠ€æœ¯æ¼”è¿›è·¯å¾„
        if llm_analysis.get("tech_evolution"):
            md.append("## ğŸ—ºï¸ æŠ€æœ¯æ¼”è¿›è·¯å¾„")
            md.append("")
            for week in llm_analysis["tech_evolution"]:
                md.append(f"- **{week['week']}**: {week['focus']}")
                md.append(f"  - å…³é”®æ”¶è·: {week['key_learning']}")
            md.append("")

        # ä¿¡æ¯æºè´¨é‡è¯„ä¼°
        if llm_analysis.get("source_quality"):
            md.append("## ğŸ“Š ä¿¡æ¯æºè´¨é‡è¯„ä¼°")
            md.append("")
            md.append("| ä¿¡æ¯æº | æœ‰æ•ˆå†…å®¹ | é«˜ä»·å€¼å æ¯” | è¯„çº§ | å»ºè®® |")
            md.append("|--------|---------|-----------|------|------|")
            for src in llm_analysis["source_quality"]:
                stars = "â­" * int(float(src.get("rating", "3")))
                rate = f"{float(src.get('high_value_rate', 0)) * 100:.0f}%"
                md.append(f"| {src['source']} | {src.get('value_count', '?')} | {rate} | {stars} | {src.get('suggestion', '-')} |")
            md.append("")

        # çŸ¥è¯†è¦†ç›–åˆ†æ
        if llm_analysis.get("knowledge_coverage"):
            kc = llm_analysis["knowledge_coverage"]
            md.append("## ğŸ§  çŸ¥è¯†è¦†ç›–åˆ†æ")
            md.append("")
            md.append(f"**å­¦ä¹ é£æ ¼è¯„ä¼°**: {kc.get('depth_vs_breadth', 'æœªçŸ¥')}")
            md.append("")
            if kc.get("covered_areas"):
                md.append(f"**å·²è¦†ç›–é¢†åŸŸ**: {', '.join(kc['covered_areas'])}")
            if kc.get("deep_areas"):
                md.append(f"**æ·±åº¦é¢†åŸŸ**: {', '.join(kc['deep_areas'])}")
            if kc.get("blind_spots"):
                md.append("")
                md.append("**âš ï¸ çŸ¥è¯†ç›²åŒº**:")
                for spot in kc["blind_spots"]:
                    md.append(f"  - {spot}")
            md.append("")

        # æˆé•¿è¯„ä¼°
        if llm_analysis.get("growth_assessment"):
            ga = llm_analysis["growth_assessment"]
            md.append("## ğŸ“ˆ æˆé•¿è¯„ä¼°")
            md.append("")
            md.append(f"**ç»¼åˆè¯„åˆ†**: {ga.get('overall_score', '?')}/100  |  **æˆé•¿æ›²çº¿**: {ga.get('growth_curve', '?')}")
            md.append("")
            if ga.get("strengths"):
                md.append("**âœ… åšå¾—å¥½çš„**:")
                for s in ga["strengths"]:
                    md.append(f"  - {s}")
            if ga.get("weaknesses"):
                md.append("")
                md.append("**âš ï¸ éœ€æ”¹è¿›çš„**:")
                for w in ga["weaknesses"]:
                    md.append(f"  - {w}")
            md.append("")

    # è¡ŒåŠ¨é¡¹å›é¡¾
    md.append("## âœ… æœˆåº¦è¡ŒåŠ¨é¡¹å›é¡¾")
    md.append("")
    if action_items["items"]:
        status_emoji = {"pending": "â³", "in_progress": "ğŸ”„", "done": "âœ…", "dropped": "ğŸ—‘ï¸"}
        md.append("| è¡ŒåŠ¨é¡¹ | æ¥æº | ä¼˜å…ˆçº§ | çŠ¶æ€ |")
        md.append("|--------|------|--------|------|")
        for item in action_items["items"][:20]:
            emoji = status_emoji.get(item["status"], "â“")
            md.append(f"| {item['title'][:35]} | {item.get('source', '-')} | {item['priority']} | {emoji} |")
        if len(action_items["items"]) > 20:
            md.append(f"| ... è¿˜æœ‰ {len(action_items['items']) - 20} é¡¹ | | | |")
    else:
        md.append("æœ¬æœˆæš‚æ— è¡ŒåŠ¨é¡¹è®°å½•")
    md.append("")

    # ä¸‹æœˆè§„åˆ’
    if llm_analysis and llm_analysis.get("next_month_plan"):
        nmp = llm_analysis["next_month_plan"]
        md.append("## ğŸ¯ ä¸‹æœˆå­¦ä¹ è§„åˆ’")
        md.append("")

        if nmp.get("focus_directions"):
            md.append("### æ¨èé‡ç‚¹æ–¹å‘")
            for i, fd in enumerate(nmp["focus_directions"], 1):
                md.append(f"**{i}. {fd['direction']}**")
                md.append(f"  - åŸå› : {fd['reason']}")
                if fd.get("resources"):
                    md.append(f"  - èµ„æº: {', '.join(fd['resources'])}")
                md.append("")

        if nmp.get("monthly_challenge"):
            mc = nmp["monthly_challenge"]
            md.append("### ğŸ† æœˆåº¦æŒ‘æˆ˜")
            md.append(f"**{mc['title']}**")
            md.append(f"  {mc.get('description', '')}")
            md.append(f"  æˆåŠŸæ ‡å‡†: {mc.get('success_criteria', 'æœªå®šä¹‰')}")
            md.append("")

        if nmp.get("avoid_pitfalls"):
            md.append("### âš ï¸ éœ€è¦é¿å…")
            for pit in nmp["avoid_pitfalls"]:
                md.append(f"  - {pit}")
            md.append("")

    md.append("---")
    md.append("*è‡ªåŠ¨ç”Ÿæˆäº Monthly Reviewer v3.0*")

    return '\n'.join(md)


def notion_request(endpoint, method='GET', data=None):
    """Notion API è¯·æ±‚"""
    url = f"{MATON_BASE_URL}/notion/{endpoint}"
    headers = {
        "Authorization": f"Bearer {MATON_API_KEY}",
        "Content-Type": "application/json",
        "Notion-Version": "2022-06-28"
    }
    ctx = ssl.create_default_context()
    try:
        req_data = json.dumps(data).encode('utf-8') if data else None
        req = urllib.request.Request(url, data=req_data, headers=headers, method=method)
        response = urllib.request.urlopen(req, context=ctx, timeout=30)
        return json.load(response)
    except Exception as e:
        print(f"âŒ Notion è¯·æ±‚å¤±è´¥: {e}")
        return None


def search_notion_page(title):
    """æœç´¢ Notion é¡µé¢"""
    result = notion_request("search", method='POST', data={
        "query": title,
        "filter": {"property": "object", "value": "page"}
    })
    if result and result.get("results"):
        for page in result["results"]:
            page_title = ""
            props = page.get("properties", {})
            if "title" in props:
                title_arr = props["title"].get("title", [])
                if title_arr:
                    page_title = title_arr[0].get("plain_text", "")
            if title in page_title:
                return page.get("id")
    return None


def create_monthly_notion_page(month_info, report_content):
    """åœ¨ Notion åˆ›å»ºæœˆåº¦å¤ç›˜é¡µé¢ï¼ˆæ”¾åœ¨æ ¹é¡µé¢ä¸‹ï¼‰"""

    children = []

    children.append({
        "object": "block",
        "type": "heading_2",
        "heading_2": {
            "rich_text": [{"type": "text", "text": {"content": f"ğŸ“ˆ {month_info['year_month_cn']} â€” æœˆåº¦å¤ç›˜"}}]
        }
    })

    children.append({
        "object": "block",
        "type": "callout",
        "callout": {
            "rich_text": [{"type": "text", "text": {"content": f"ç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M')}"}}],
            "icon": {"emoji": "ğŸ“ˆ"}
        }
    })

    # å°†æŠ¥å‘Šå†…å®¹è½¬ä¸º Notion blocks
    for line in report_content.split('\n'):
        line = line.strip()
        if not line:
            continue
        if line.startswith('# '):
            continue
        elif line.startswith('## '):
            children.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": line[3:]}}]
                }
            })
        elif line.startswith('### '):
            children.append({
                "object": "block",
                "type": "heading_3",
                "heading_3": {
                    "rich_text": [{"type": "text", "text": {"content": line[4:]}}]
                }
            })
        elif line.startswith('- '):
            children.append({
                "object": "block",
                "type": "bulleted_list_item",
                "bulleted_list_item": {
                    "rich_text": [{"type": "text", "text": {"content": line[2:][:2000]}}]
                }
            })
        elif line.startswith('|') and '---' not in line:
            children.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": line[:2000]}}]
                }
            })
        elif line == '---':
            children.append({
                "object": "block",
                "type": "divider",
                "divider": {}
            })
        elif len(line) > 2:
            children.append({
                "object": "block",
                "type": "paragraph",
                "paragraph": {
                    "rich_text": [{"type": "text", "text": {"content": line[:2000]}}]
                }
            })

    children = children[:95]

    page_title = f"ğŸ“ˆ {month_info['year_month_cn']} â€” æœˆåº¦å¤ç›˜"

    page_data = {
        "parent": {"page_id": LEARNING_DIARY_ROOT_ID},
        "properties": {
            "title": [{"type": "text", "text": {"content": page_title}}]
        },
        "children": children
    }

    result = notion_request("pages", method='POST', data=page_data)
    return result


def update_growth_metrics(month_info, daily_stats, action_items, llm_analysis):
    """æ›´æ–°æˆé•¿æŒ‡æ ‡"""
    metrics_file = TRACKER_DIR / "growth-metrics.json"

    if metrics_file.exists():
        with open(metrics_file, 'r', encoding='utf-8') as f:
            metrics = json.load(f)
    else:
        metrics = {
            "monthly_stats": [],
            "updated_at": None
        }

    month_entry = {
        "month": month_info["year_month"],
        "learning_days": daily_stats["learning_days"],
        "total_days": daily_stats["total_days"],
        "learning_rate": daily_stats["rate"],
        "max_streak": daily_stats["max_streak"],
        "action_items_total": action_items["total"],
        "action_items_done": action_items["done"],
        "completion_rate": action_items["completion_rate"],
        "overall_score": llm_analysis.get("growth_assessment", {}).get("overall_score") if llm_analysis else None,
        "recorded_at": datetime.now().isoformat()
    }

    # é¿å…é‡å¤
    metrics["monthly_stats"] = [
        m for m in metrics.get("monthly_stats", [])
        if m.get("month") != month_info["year_month"]
    ]
    metrics["monthly_stats"].append(month_entry)
    metrics["updated_at"] = datetime.now().isoformat()

    TRACKER_DIR.mkdir(parents=True, exist_ok=True)
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, ensure_ascii=False, indent=2)


# === ä¸»æµç¨‹ ===

def main():
    print("=" * 60)
    print("ğŸ“ˆ Learning Upgrade â€” æ¯æœˆå¤ç›˜åˆ†æå™¨ v3.0")
    print("=" * 60)

    load_env()

    month_info = get_last_month_info()
    print(f"\nğŸ“… å¤ç›˜æœˆä»½: {month_info['year_month_cn']}")
    print(f"   æ—¥æœŸèŒƒå›´: {month_info['first_day'].strftime('%Y-%m-%d')} ~ {month_info['last_day'].strftime('%Y-%m-%d')}")

    # Step 1: åŠ è½½å‘¨æŠ¥
    print(f"\nğŸ“¥ æ­¥éª¤ 1/6: åŠ è½½ä¸Šæœˆå‘¨æŠ¥...")
    weekly_reports = load_weekly_reports(month_info["year"], month_info["month"])
    print(f"  âœ… åŠ è½½ {len(weekly_reports)} ä»½å‘¨æŠ¥")

    # Step 2: ç»Ÿè®¡æ¯æ—¥å­¦ä¹ 
    print(f"\nğŸ“Š æ­¥éª¤ 2/6: ç»Ÿè®¡æ¯æ—¥å­¦ä¹ æƒ…å†µ...")
    daily_stats = load_daily_stats(month_info["year"], month_info["month"])
    print(f"  å­¦ä¹ å¤©æ•°: {daily_stats['learning_days']}/{daily_stats['total_days']}")
    print(f"  æœ€ä½³è¿ç»­: {daily_stats['max_streak']} å¤©")

    # Step 3: åŠ è½½è¡ŒåŠ¨é¡¹
    print(f"\nâœ… æ­¥éª¤ 3/6: åŠ è½½æœˆåº¦è¡ŒåŠ¨é¡¹...")
    action_items = load_monthly_action_items(month_info["year_month"])
    print(f"  æ€»è®¡: {action_items['total']}  å®Œæˆ: {action_items['done']}  å®Œæˆç‡: {action_items['completion_rate'] * 100:.0f}%")

    # Step 4: LLM åˆ†æ
    print(f"\nğŸ¤– æ­¥éª¤ 4/6: LLM æœˆåº¦ç»¼åˆåˆ†æ...")
    llm_analysis = llm_monthly_analysis(weekly_reports, daily_stats, action_items, month_info)
    if llm_analysis:
        print("  âœ… åˆ†æå®Œæˆ")
        if llm_analysis.get("growth_assessment"):
            print(f"    ç»¼åˆè¯„åˆ†: {llm_analysis['growth_assessment'].get('overall_score', '?')}/100")
    else:
        print("  âš ï¸ LLM åˆ†æå¤±è´¥ï¼Œä½¿ç”¨åŸºç¡€æ•°æ®")

    # Step 5: ç”ŸæˆæŠ¥å‘Š & Notion
    print(f"\nğŸ“ æ­¥éª¤ 5/6: ç”ŸæˆæœˆæŠ¥ & Notion æ›´æ–°...")

    report_md = generate_monthly_report(
        month_info, daily_stats, weekly_reports, action_items, llm_analysis
    )

    # ä¿å­˜æœ¬åœ°
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    report_file = OUTPUT_DIR / f"{month_info['year_month']}.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report_md)
    print(f"  âœ… æœ¬åœ°æŠ¥å‘Š: {report_file}")

    if llm_analysis:
        json_file = OUTPUT_DIR / f"{month_info['year_month']}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(llm_analysis, f, ensure_ascii=False, indent=2)

    # æ›´æ–°æˆé•¿æŒ‡æ ‡
    update_growth_metrics(month_info, daily_stats, action_items, llm_analysis)
    print(f"  âœ… æˆé•¿æŒ‡æ ‡å·²æ›´æ–°")

    # Notion æœˆåº¦å¤ç›˜
    monthly_title = f"{month_info['year_month_cn']} â€” æœˆåº¦å¤ç›˜"
    existing = search_notion_page(monthly_title)
    if existing:
        print(f"  âš ï¸ æœˆåº¦å¤ç›˜é¡µé¢å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º")
    else:
        result = create_monthly_notion_page(month_info, report_md)
        if result:
            page_id = result.get('id', '')
            print(f"  âœ… Notion æœˆæŠ¥åˆ›å»ºæˆåŠŸ: {page_id}")
        else:
            print(f"  âŒ Notion æœˆæŠ¥åˆ›å»ºå¤±è´¥")

    # Step 6: Telegram æ‘˜è¦
    print(f"\nğŸ“± æ­¥éª¤ 6/6: Telegram æ‘˜è¦...")
    tg_summary = generate_telegram_summary(month_info, daily_stats, action_items, llm_analysis)
    print(tg_summary)

    print(f"\n{'=' * 60}")
    print(f"ğŸ‰ æœˆåº¦å¤ç›˜å®Œæˆï¼({month_info['year_month']})")
    print(f"{'=' * 60}")


def generate_telegram_summary(month_info, daily_stats, action_items, llm_analysis):
    """ç”Ÿæˆ Telegram æ¨é€æ‘˜è¦"""
    lines = []
    lines.append(f"ğŸ“ˆ {month_info['year_month_cn']} æœˆåº¦å¤ç›˜å®Œæˆ")
    lines.append("")
    lines.append(f"ğŸ“… å­¦ä¹ å¤©æ•°: {daily_stats['learning_days']}/{daily_stats['total_days']} ({daily_stats['rate'] * 100:.0f}%)")
    lines.append(f"ğŸ”¥ æœ€ä½³è¿ç»­: {daily_stats['max_streak']} å¤©")
    lines.append(f"âœ… è¡ŒåŠ¨é¡¹å®Œæˆç‡: {action_items['completion_rate'] * 100:.0f}%")

    if llm_analysis:
        ga = llm_analysis.get("growth_assessment", {})
        if ga.get("overall_score"):
            lines.append(f"ğŸ“Š ç»¼åˆè¯„åˆ†: {ga['overall_score']}/100 ({ga.get('growth_curve', '')})")

        nmp = llm_analysis.get("next_month_plan", {})
        focus = nmp.get("focus_directions", [])
        if focus:
            lines.append("")
            lines.append("ğŸ¯ ä¸‹æœˆé‡ç‚¹:")
            for fd in focus[:3]:
                lines.append(f"  â€¢ {fd['direction']}")

        challenge = nmp.get("monthly_challenge", {})
        if challenge.get("title"):
            lines.append(f"\nğŸ† æœˆåº¦æŒ‘æˆ˜: {challenge['title']}")

    return '\n'.join(lines)


if __name__ == "__main__":
    main()
