#!/usr/bin/env python3
"""
æŠ€æœ¯æ·±åº¦åˆ†æå™¨ v3.0
å˜æ›´ï¼šåœ¨åŸæœ‰åˆ†æåŸºç¡€ä¸Šå¢åŠ  action_items è¾“å‡º
     è¡ŒåŠ¨é¡¹è‡ªåŠ¨å†™å…¥ tracker/action-items.json
"""

import json
import os
import ssl
import sys
import urllib.request
from datetime import datetime
from pathlib import Path

# === è·¯å¾„é…ç½® ===
WORKSPACE_DIR = Path("/home/writer/.openclaw/workspace")
LOGS_DIR = WORKSPACE_DIR / "logs"
OUTPUT_DIR = LOGS_DIR / "tech-analyzer"
SKILL_DIR = WORKSPACE_DIR / "skills" / "learning-upgrade"

# === API é…ç½® ===
ARK_API_KEY = os.environ.get('ARK_API_KEY', '')
ARK_BASE_URL = "https://ark.cn-beijing.volces.com/api/coding/v3"

def load_env():
    """åŠ è½½ç¯å¢ƒå˜é‡"""
    env_file = Path("/home/writer/.openclaw/.env")
    if env_file.exists():
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    if line.startswith('export '):
                        line = line[7:]
                    key, _, value = line.partition('=')
                    key = key.strip()
                    value = value.strip().strip('"').strip("'")
                    os.environ[key] = value
    global ARK_API_KEY
    ARK_API_KEY = os.environ.get('ARK_API_KEY', ARK_API_KEY)


def load_daily_reports():
    """åŠ è½½å½“æ—¥çš„ GitHub å’Œç¤¾åŒºæŠ¥å‘Š"""
    today = datetime.now().strftime('%Y%m%d')
    reports = {}

    gh_file = LOGS_DIR / "github-monitor" / f"github-monitor-{today}.md"
    if gh_file.exists():
        with open(gh_file, 'r', encoding='utf-8') as f:
            reports['github'] = f.read()[:3000]
        print("  âœ… åŠ è½½ GitHub æŠ¥å‘Š")

    comm_file = LOGS_DIR / "community-scraper" / f"community-scraper-{today}.md"
    if comm_file.exists():
        with open(comm_file, 'r', encoding='utf-8') as f:
            reports['community'] = f.read()[:3000]
        print("  âœ… åŠ è½½ç¤¾åŒºæŠ¥å‘Š")

    # JSON æ ¼å¼çš„ GitHub æŠ¥å‘Šï¼ˆæ›´ç»“æ„åŒ–ï¼‰
    gh_json = LOGS_DIR / "github-monitor" / f"github-monitor-{today}.json"
    if gh_json.exists():
        with open(gh_json, 'r', encoding='utf-8') as f:
            try:
                reports['github_json'] = json.load(f)
            except json.JSONDecodeError:
                pass

    comm_json = LOGS_DIR / "community-scraper" / f"community-scraper-{today}.json"
    if comm_json.exists():
        with open(comm_json, 'r', encoding='utf-8') as f:
            try:
                reports['community_json'] = json.load(f)
            except json.JSONDecodeError:
                pass

    return reports


def extract_technical_content(reports):
    """æå–æŠ€æœ¯å†…å®¹"""
    content = []

    # ä» GitHub JSON æå–
    if 'github_json' in reports:
        gh = reports['github_json']
        repos = gh.get('repos', {})

        # ä¸»ä»“åº“æ•°æ®
        main_repo = repos.get('main', {})
        for rel in main_repo.get('releases', []):
            content.append({
                "source": "GitHub Release",
                "title": f"{rel['tag']} - {rel['name']}",
                "date": rel.get('published_at', ''),
                "details": rel.get('body', '')[:500]
            })

        for topic in main_repo.get('trending_topics', []):
            content.append({
                "source": "GitHub Issue",
                "title": topic.get('title', ''),
                "comments": topic.get('comments', 0),
                "labels": ', '.join(topic.get('labels', []))
            })

        if main_repo.get('stars'):
            content.append({
                "source": "GitHub Stats",
                "stars": main_repo['stars'].get('stars', 0),
                "forks": main_repo['stars'].get('forks', 0),
                "open_issues": main_repo['stars'].get('open_issues', 0)
            })

    # ä»ç¤¾åŒº JSON æå–
    if 'community_json' in reports:
        comm = reports['community_json']
        sources = comm.get('sources', {})

        awesome = sources.get('awesome-openclaw', {})
        if awesome:
            content.append({
                "source": "awesome-openclaw",
                "total_resources": awesome.get('total_resources', 0),
                "categories": awesome.get('category_count', 0)
            })

        hn = sources.get('hacker-news', {})
        for story in hn.get('ai_stories', [])[:5]:
            content.append({
                "source": "Hacker News",
                "title": story.get('title', ''),
                "score": story.get('score', 0),
                "comments": story.get('comments', 0)
            })

        clawhub = sources.get('clawhub', {})
        if clawhub:
            content.append({
                "source": "ClawHub",
                "stars": clawhub.get('stars', 0),
                "forks": clawhub.get('forks', 0)
            })

    return content


def analyze_with_llm(technical_content):
    """ä½¿ç”¨ LLM è¿›è¡ŒæŠ€æœ¯æ·±åº¦åˆ†æ (v3.0: å¢åŠ  action_items)"""

    prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AI æ¶æ„å¸ˆå’ŒæŠ€æœ¯åˆ†æå¸ˆã€‚è¯·åˆ†æä»¥ä¸‹ OpenClaw æŠ€æœ¯åŠ¨æ€ï¼Œå¹¶è¾“å‡ºæ·±åº¦æ´å¯Ÿï¼š

## æŠ€æœ¯å†…å®¹
"""

    for item in technical_content[:10]:
        prompt += f"\n### {item['source']}\n"
        for key, value in item.items():
            if key != 'source':
                prompt += f"- {key}: {value}\n"

    prompt += """

## åˆ†æè¦æ±‚

è¯·æŒ‰ä»¥ä¸‹ç»´åº¦è¾“å‡ºåˆ†æç»“æœï¼ˆJSON æ ¼å¼ï¼‰ï¼š

```json
{
  "architecture_highlights": [
    {
      "title": "æ¶æ„è®¾è®¡äº®ç‚¹",
      "description": "è¯¦ç»†æè¿°",
      "impact": "é«˜/ä¸­/ä½",
      "relevance_to_us": "ä¸æˆ‘ä»¬å½“å‰æ¶æ„çš„ç›¸å…³æ€§"
    }
  ],
  "security_trends": [
    {
      "trend": "å®‰å…¨è¶‹åŠ¿",
      "details": "è¯¦ç»†è¯´æ˜",
      "priority": "P0/P1/P2",
      "action_required": "æ˜¯å¦éœ€è¦ç«‹å³è¡ŒåŠ¨"
    }
  ],
  "performance_optimizations": [
    {
      "area": "æ€§èƒ½ä¼˜åŒ–é¢†åŸŸ",
      "technique": "æŠ€æœ¯æ–¹æ³•",
      "estimated_improvement": "é¢„ä¼°æå‡"
    }
  ],
  "community_patterns": [
    {
      "pattern": "ç¤¾åŒºæ¨¡å¼",
      "evidence": "è¯æ®",
      "implication": "å¯¹æˆ‘ä»¬çš„å¯ç¤º"
    }
  ],
  "technical_debt_risks": [
    {
      "risk": "æŠ€æœ¯å€ºåŠ¡é£é™©",
      "severity": "ä¸¥é‡/ä¸­ç­‰/è½»å¾®",
      "mitigation": "ç¼“è§£æªæ–½"
    }
  ],
  "innovation_opportunities": [
    {
      "opportunity": "åˆ›æ–°æœºä¼š",
      "feasibility": "å¯è¡Œæ€§ï¼ˆé«˜/ä¸­/ä½ï¼‰",
      "effort": "é¢„è®¡å·¥ä½œé‡",
      "value": "ä¸šåŠ¡ä»·å€¼"
    }
  ],
  "action_items": [
    {
      "title": "å…·ä½“è¡ŒåŠ¨é¡¹æ ‡é¢˜",
      "priority": "high/medium/low",
      "steps": ["æ­¥éª¤1", "æ­¥éª¤2", "æ­¥éª¤3"],
      "expected_days": 7,
      "reason": "ä¸ºä»€ä¹ˆéœ€è¦åšè¿™ä»¶äº‹"
    }
  ]
}
```

è¯·ç¡®ä¿ï¼š
1. åˆ†ææ·±å…¥ã€å…·ä½“ã€å¯æ‰§è¡Œ
2. action_items æ˜¯ä½ ä»åˆ†æä¸­æç‚¼å‡ºçš„æœ€é‡è¦çš„ 2-3 ä¸ªæ”¹è¿›è¡ŒåŠ¨
3. æ¯ä¸ª action_item å¿…é¡»æœ‰å…·ä½“çš„æ‰§è¡Œæ­¥éª¤
"""

    url = f"{ARK_BASE_URL}/chat/completions"
    headers = {
        "Authorization": f"Bearer {ARK_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "glm-4.7",
        "messages": [
            {
                "role": "system",
                "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ AI æ¶æ„å¸ˆå’ŒæŠ€æœ¯åˆ†æå¸ˆï¼Œæ“…é•¿ä»æŠ€æœ¯åŠ¨æ€ä¸­æå–æ·±åº¦æ´å¯Ÿå’Œæ¶æ„ä¼˜åŒ–å»ºè®®ã€‚"
            },
            {
                "role": "user",
                "content": prompt
            }
        ],
        "temperature": 0.7,
        "max_tokens": 4000
    }

    ctx = ssl.create_default_context()

    try:
        req = urllib.request.Request(
            url,
            data=json.dumps(payload).encode('utf-8'),
            headers=headers,
            method='POST'
        )

        response = urllib.request.urlopen(req, context=ctx, timeout=180)
        result = json.load(response)
        content = result['choices'][0]['message']['content']

        import re
        json_match = re.search(r'```json\s*(.*?)\s*```', content, re.DOTALL)
        if json_match:
            analysis = json.loads(json_match.group(1))
        else:
            analysis = json.loads(content)

        return analysis

    except Exception as e:
        print(f"âŒ LLM åˆ†æå¤±è´¥ï¼š{e}")
        return None


def save_action_items(analysis):
    """å°†åˆ†æç»“æœä¸­çš„è¡ŒåŠ¨é¡¹ä¿å­˜åˆ° tracker (v3.0 æ–°å¢)"""
    action_items = analysis.get("action_items", [])
    if not action_items:
        print("  â„¹ï¸ æœ¬æ¬¡åˆ†ææ— è¡ŒåŠ¨é¡¹è¾“å‡º")
        return

    sys.path.insert(0, str(SKILL_DIR / "tools"))
    try:
        import importlib
        # åŠ¨æ€å¯¼å…¥ï¼ˆæ–‡ä»¶ååŒ…å«è¿å­—ç¬¦ï¼‰
        spec = importlib.util.spec_from_file_location(
            "action_tracker",
            SKILL_DIR / "tools" / "action-tracker.py"
        )
        at = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(at)

        for item in action_items[:3]:  # æ¯å¤©æœ€å¤š 3 ä¸ªè¡ŒåŠ¨é¡¹
            at.add_item(
                title=item.get("title", "æœªå‘½å"),
                priority=item.get("priority", "medium"),
                source="daily",
                steps=item.get("steps", []),
                expected_days=item.get("expected_days", 7)
            )
        print(f"  âœ… å·²ä¿å­˜ {min(len(action_items), 3)} ä¸ªè¡ŒåŠ¨é¡¹åˆ° tracker")
    except Exception as e:
        print(f"  âš ï¸ ä¿å­˜è¡ŒåŠ¨é¡¹å¤±è´¥: {e}")
        # é™çº§: ç›´æ¥å†™å…¥ JSON
        try:
            tracker_file = SKILL_DIR / "tracker" / "action-items.json"
            tracker_file.parent.mkdir(parents=True, exist_ok=True)

            if tracker_file.exists():
                with open(tracker_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                data = {"items": [], "stats": {}}

            today = datetime.now().strftime('%Y-%m-%d')
            week_num = datetime.now().isocalendar()[1]

            for i, item in enumerate(action_items[:3]):
                data["items"].append({
                    "id": f"AI-{today.replace('-', '')}-{len(data['items']) + 1:03d}",
                    "title": item.get("title", ""),
                    "source": "daily",
                    "source_date": today,
                    "priority": item.get("priority", "medium"),
                    "status": "pending",
                    "steps": item.get("steps", []),
                    "created_at": datetime.now().isoformat(),
                    "completed_at": None,
                    "review_week": f"{datetime.now().year}-W{week_num:02d}"
                })

            with open(tracker_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
            print(f"  âœ… é™çº§ä¿å­˜è¡ŒåŠ¨é¡¹æˆåŠŸ")
        except Exception as e2:
            print(f"  âŒ é™çº§ä¿å­˜ä¹Ÿå¤±è´¥: {e2}")


def generate_tech_insight_report(analysis):
    """ç”ŸæˆæŠ€æœ¯æ´å¯ŸæŠ¥å‘Š"""
    if not analysis:
        return "âŒ LLM åˆ†æå¤±è´¥"

    report = []
    report.append("# æŠ€æœ¯æ·±åº¦æ´å¯ŸæŠ¥å‘Š")
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    report.append("")

    # æ¶æ„è®¾è®¡äº®ç‚¹
    if 'architecture_highlights' in analysis:
        report.append("## ğŸ—ï¸ æ¶æ„è®¾è®¡äº®ç‚¹")
        report.append("")
        for i, highlight in enumerate(analysis['architecture_highlights'], 1):
            report.append(f"### {i}. {highlight['title']}")
            report.append(f"**å½±å“**: {highlight.get('impact', 'æœªçŸ¥')}")
            report.append(f"**ç›¸å…³æ€§**: {highlight.get('relevance_to_us', 'æœªçŸ¥')}")
            report.append("")
            report.append(highlight['description'])
            report.append("")

    # å®‰å…¨è¶‹åŠ¿
    if 'security_trends' in analysis:
        report.append("## ğŸ”’ å®‰å…¨è¶‹åŠ¿")
        report.append("")
        for trend in analysis['security_trends']:
            report.append(f"- **{trend['trend']}** [{trend['priority']}]")
            report.append(f"  - {trend['details']}")
            if trend.get('action_required'):
                report.append(f"  - âš ï¸ **éœ€è¦ç«‹å³è¡ŒåŠ¨**")
            report.append("")

    # æ€§èƒ½ä¼˜åŒ–
    if 'performance_optimizations' in analysis:
        report.append("## âš¡ æ€§èƒ½ä¼˜åŒ–")
        report.append("")
        for opt in analysis['performance_optimizations']:
            report.append(f"- **{opt['area']}**")
            report.append(f"  - æŠ€æœ¯ï¼š{opt['technique']}")
            report.append(f"  - é¢„ä¼°æå‡ï¼š{opt.get('estimated_improvement', 'æœªçŸ¥')}")
            report.append("")

    # ç¤¾åŒºæ¨¡å¼
    if 'community_patterns' in analysis:
        report.append("## ğŸ‘¥ ç¤¾åŒºæ¨¡å¼")
        report.append("")
        for pattern in analysis['community_patterns']:
            report.append(f"- **{pattern['pattern']}**")
            report.append(f"  - è¯æ®ï¼š{pattern.get('evidence', 'æ— ')}")
            report.append(f"  - å¯ç¤ºï¼š{pattern.get('implication', 'æ— ')}")
            report.append("")

    # æŠ€æœ¯å€ºåŠ¡é£é™©
    if 'technical_debt_risks' in analysis:
        report.append("## âš ï¸ æŠ€æœ¯å€ºåŠ¡é£é™©")
        report.append("")
        for risk in analysis['technical_debt_risks']:
            report.append(f"- **{risk['risk']}** [{risk['severity']}]")
            report.append(f"  - ç¼“è§£ï¼š{risk.get('mitigation', 'æ— ')}")
            report.append("")

    # åˆ›æ–°æœºä¼š
    if 'innovation_opportunities' in analysis:
        report.append("## ğŸ’¡ åˆ›æ–°æœºä¼š")
        report.append("")
        for opp in analysis['innovation_opportunities']:
            report.append(f"- **{opp['opportunity']}**")
            report.append(f"  - å¯è¡Œæ€§ï¼š{opp.get('feasibility', 'æœªçŸ¥')}")
            report.append(f"  - å·¥ä½œé‡ï¼š{opp.get('effort', 'æœªçŸ¥')}")
            report.append(f"  - ä»·å€¼ï¼š{opp.get('value', 'æœªçŸ¥')}")
            report.append("")

    # è¡ŒåŠ¨é¡¹ (v3.0 æ–°å¢)
    if 'action_items' in analysis:
        report.append("## ğŸ“‹ ä»Šæ—¥è¡ŒåŠ¨é¡¹")
        report.append("")
        for item in analysis['action_items']:
            priority_map = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}
            emoji = priority_map.get(item.get("priority", "medium"), "ğŸŸ¡")
            report.append(f"### {emoji} {item['title']}")
            report.append(f"**åŸå› **: {item.get('reason', 'æœªè¯´æ˜')}")
            if item.get("steps"):
                for step in item["steps"]:
                    report.append(f"  - [ ] {step}")
            report.append("")

    return '\n'.join(report)


def main():
    print("ğŸ” åŠ è½½æ¯æ—¥æŠ¥å‘Š...")
    load_env()
    reports = load_daily_reports()

    if not reports:
        print("âŒ æœªæ‰¾åˆ°æ¯æ—¥æŠ¥å‘Š")
        return

    print("\nğŸ“Š æå–æŠ€æœ¯å†…å®¹...")
    tech_content = extract_technical_content(reports)
    print(f"  æå– {len(tech_content)} æ¡æŠ€æœ¯å†…å®¹")

    print("\nğŸ¤– è°ƒç”¨ LLM è¿›è¡Œæ·±åº¦åˆ†æ...")
    analysis = analyze_with_llm(tech_content)

    if not analysis:
        print("âŒ LLM åˆ†æå¤±è´¥")
        return

    print(f"  âœ… åˆ†æå®Œæˆ")
    print(f"  - æ¶æ„äº®ç‚¹ï¼š{len(analysis.get('architecture_highlights', []))} ä¸ª")
    print(f"  - å®‰å…¨è¶‹åŠ¿ï¼š{len(analysis.get('security_trends', []))} ä¸ª")
    print(f"  - æ€§èƒ½ä¼˜åŒ–ï¼š{len(analysis.get('performance_optimizations', []))} ä¸ª")
    print(f"  - åˆ›æ–°æœºä¼šï¼š{len(analysis.get('innovation_opportunities', []))} ä¸ª")
    print(f"  - è¡ŒåŠ¨é¡¹ï¼š{len(analysis.get('action_items', []))} ä¸ª")

    # v3.0: ä¿å­˜è¡ŒåŠ¨é¡¹åˆ° tracker
    print("\nğŸ“‹ ä¿å­˜è¡ŒåŠ¨é¡¹...")
    save_action_items(analysis)

    print("\nğŸ“ ç”ŸæˆæŠ€æœ¯æ´å¯ŸæŠ¥å‘Š...")
    report = generate_tech_insight_report(analysis)

    # ä¿å­˜æŠ¥å‘Š
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    today = datetime.now().strftime('%Y%m%d')
    output_file = OUTPUT_DIR / f"tech-analysis-{today}.md"
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report)
    print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜ï¼š{output_file}")

    # ä¿å­˜ JSON åˆ†æç»“æœ
    json_file = OUTPUT_DIR / f"tech-analysis-{today}.json"
    with open(json_file, 'w', encoding='utf-8') as f:
        json.dump(analysis, f, ensure_ascii=False, indent=2)
    print(f"âœ… JSON å·²ä¿å­˜ï¼š{json_file}")

    print("\nğŸ‰ æŠ€æœ¯æ·±åº¦åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()
